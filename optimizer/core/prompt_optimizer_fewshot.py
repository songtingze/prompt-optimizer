from llm.llm_t import LLM
from llm.llm_config import LLM_Config
from optimizer.utils.evaluation_utils import extract_content, list_to_markdown, EvaluationUtils
from optimizer.prompts.prompt_pool import PROMPT_OPTIMIZE_PROMPT
from utils.logger import logger

# from optimizer.prompts.prompt_template import BROKE_TEMPLATE

class PromptOptimizer:
    def __init__(
            self,
            optimize_llm_config: LLM_Config,
            evaluate_llm_config: LLM_Config,
            execute_llm_config: LLM_Config,
            # rounds: int,
            # optimized_path: str = None,
            # name: str = "",
            # template: str = ""
    ):
        """
        åˆå§‹åŒ–æç¤ºè¯ä¼˜åŒ–å™¨

        Args:
            model_name: æ¨¡å‹åç§°
            api_key: APIå¯†é’¥
            base_url: APIåŸºç¡€URL
            temperature: æ¨¡å‹temperature
            **kwargs: å…¶ä»–æ¨¡å‹å‚æ•°
        """
        self.optimize_llm = LLM(
            model_name=optimize_llm_config.model_name,
            api_key=optimize_llm_config.api_key,
            base_url=optimize_llm_config.base_url,
            temperature=optimize_llm_config.temperature
        )

        self.evaluate_llm = LLM(
            model_name=evaluate_llm_config.model_name,
            api_key=evaluate_llm_config.api_key,
            base_url=evaluate_llm_config.base_url,
            temperature=evaluate_llm_config.temperature
        )

        self.execute_llm = LLM(
            model_name=execute_llm_config.model_name,
            api_key=execute_llm_config.api_key,
            base_url=execute_llm_config.base_url,
            temperature=execute_llm_config.temperature
        )

        # self.name = name
        # self.root_path = Path(optimized_path) / self.name
        self.top_scores = []
        # self.round = rounds
        # self.max_rounds = max_rounds
        # self.template = template
        # self.prompt_utils = PromptUtils(self.root_path)
        self.evaluation_utils = EvaluationUtils()
        # self.data_utils = DataUtils(self.root_path)
        # self.evaluation_utils = EvaluationUtils(self.root_path)

    # ç¬¬ä¸€è½®ä¼˜åŒ–è¿‡ç¨‹ todo:åˆå§‹æ¥æ”¶é—®ç­”å¯¹
    def optimize_first(self, current_prompt, qa_list):
        # 2.æ‰§è¡Œä¼˜åŒ–åçš„prompt
        new_samples = self._execute_prompt(current_prompt, qa_list)
        # 3.è¯„ä¼°å¹¶åæ€
        modification_all, anaysis_all, _ = self._evaluate_prompt(new_samples, None, qa_list, initial=True)
        # self._run_async_evaluate(new_samples, initial=True)
        # self.round += 1
        # best_samples = self.data_utils.get_best_round()
        # 1.ä¼˜åŒ–æ›´æ–°prompt(åŸºäºä¸Šä¸€è½®çš„prompt)
        new_prompt = self._optimize_prompt(modification_all, anaysis_all, new_samples["answers"], new_samples["prompt"],
                                           qa_list)
        # 2.æ‰§è¡Œä¼˜åŒ–åçš„prompt
        last_samples = new_samples
        new_samples = self._execute_prompt(new_prompt, qa_list)
        # 3.è¯„ä¼°å¹¶åæ€
        modification_all, anaysis_all, success = self._evaluate_prompt(new_samples, last_samples, qa_list)
        # ç»“æœå±•ç¤ºå¹¶è¿”å›æœ€æ–°
        # best_round = self.data_utils.get_best_round()
        self.show_final_result(success, new_samples)
        return modification_all, anaysis_all, new_prompt, new_samples['answers'], success

    # å¾ªç¯ä¼˜åŒ–è¿‡ç¨‹ todo:åˆå§‹æ¥æ”¶é—®ç­”å¯¹
    def optimize_next(self, modification_all: str, anaysis_all: str, last_answer, last_prompt, qa_list):
        # self.round = current_round
        # 1.ä¼˜åŒ–æ›´æ–°prompt(ä½¿ç”¨ä¸¤ä¸ªæ¥æºåé¦ˆ+ä¹‹å‰çš„æœ€ä½³promptå’Œå¯¹åº”å¾—åˆ°çš„ç¤ºä¾‹ç­”æ¡ˆ)
        new_prompt = self._optimize_prompt(modification_all, anaysis_all, last_answer, last_prompt, qa_list)
        # 2.æ‰§è¡Œä¼˜åŒ–åçš„prompt
        new_samples = self._execute_prompt(new_prompt, qa_list)
        # 3.è¯„ä¼°å¹¶åæ€
        last_sample = {
            "prompt": last_prompt,
            "answers": last_answer
        }
        modification_all, anaysis_all, success = self._evaluate_prompt(new_samples, last_sample, qa_list)
        # ç›®å‰æœ€æ–°ç»“æœ
        # best_round = self.data_utils.get_best_round()
        self.show_final_result(success, new_samples)
        return modification_all, anaysis_all, new_prompt, new_samples['answers'], success

    def _optimize_prompt(self, modification_all, anaysis_all, answer, current_prompt, qa_list):
        new_prompt = self._generate_optimized_prompt(modification_all, anaysis_all, answer, current_prompt, qa_list)
        self.prompt = new_prompt
        logger.info(f"\n New Prompt: {new_prompt}\n")
        return new_prompt

        # # ä¿å­˜ä¼˜åŒ–åprompt
        # directory = self.prompt_utils.create_round_directory(self.root_path, self.round)
        # self.prompt_utils.write_prompt(directory, prompt=self.prompt)

    def show_final_result(self, success, new_samples):
        # best_round = self.data_utils.get_best_round()

        logger.info("\n" + "=" * 50)
        logger.info("\nğŸ† OPTIMIZATION COMPLETED - FINAL RESULTS ğŸ†\n")
        logger.info(f"\n Optimization: {'âœ… SUCCESS' if success else 'âŒ FAILED'}\n")
        logger.info(f"\nâœ¨ Final Optimized Prompt:\n{new_samples['prompt']}")
        logger.info(f"\nğŸ¯ According Answer:\n{new_samples['answers']}")
        logger.info("\n" + "=" * 50 + "\n")

    # def _optimize_prompt_next(self, llm_feedback, human_feedback, answer, best_prompt):
    #     new_prompt = self._generate_optimized_prompt(modification_all, anaysis_all, answer, best_prompt)
    #     self.prompt = new_prompt
    #
    #     # ä¿å­˜ä¼˜åŒ–åprompt
    #     directory = self.prompt_utils.create_round_directory(self.root_path, self.round)
    #     self.prompt_utils.write_prompt(directory, prompt=self.prompt)

    def _generate_optimized_prompt(self, modification_all, anaysis_all, answer, current_prompt, qa):
        # _, requirements, qa, count = load.load_meta_data()

        logger.info(f"\nğŸš€OPTIMIZATION STARTING ğŸš€\n")
        logger.info(f"\nSelecting prompt and advancing to the iteration phase\n")

        golden_answer = list_to_markdown(qa)
        best_answer = list_to_markdown(answer)

        # ç¬¬ä¸€è½®ä¼˜åŒ–ä¸è¡¥å……ç”¨æˆ·åé¦ˆ
        logger.info(f"Feedback: {anaysis_all}\n\n{modification_all}")

        # optimize_prompt = PROMPT_OPTIMIZE_PROMPT.format(
        #     prompt=current_prompt,
        #     answers=best_answer,
        #     requirements=requirements,
        #     golden_answers=golden_answer,
        #     count=count,
        #     anaysis_all=anaysis_all,
        #     modification_all=modification_all
        #     # template=BROKE_TEMPLATE
        # )
        optimize_prompt = PROMPT_OPTIMIZE_PROMPT.format(
            prompt=current_prompt,
            answers=best_answer,
            golden_answers=golden_answer,
            anaysis_all=anaysis_all,
            modification_all=modification_all,
            count=""
            # template=BROKE_TEMPLATE
        )

        response, _ = self.optimize_llm.generate_response(optimize_prompt)
        new_prompt = extract_content(response, "prompt")

        return new_prompt if new_prompt else ""

    def _execute_prompt(self, current_prompt, qa_list):
        # load.set_file_name(self.template)
        # # ç¬¬ä¸€è½®æ²¡æœ‰ä¼˜åŒ–åçš„prompt
        # if initial:
        #     # è¯»å–åˆå§‹åŒ–é…ç½®æ¨¡æ¿
        #     prompt, _, _, _ = load.load_meta_data()
        #     # åˆå§‹è¾“å…¥prompt
        #     self.prompt = prompt
        #     current_prompt = self.prompt
        # else:
        #     # è¯»å–ä¸Šä¸€è½®ä¼˜åŒ–åçš„prompt
        #     # directory = self.prompt_utils.create_round_directory(self.root_path, self.round)
        #     # current_prompt = self.prompt_utils.load_prompt(self.round, directory)
        #     current_prompt = self.prompt
        logger.info("\nâš¡ RUNNING OPTIMIZED PROMPT âš¡\n")
        logger.info(f"\n Current_prompt:{current_prompt}\n")

        # æ‰§è¡Œprompt
        # new_samples = self._run_async_execute(current_prompt)
        new_samples = self.evaluation_utils.execute_prompt(self, current_prompt, qa_list)
        return new_samples

    def _evaluate_prompt(self, new_samples, last_samples, qa_list, initial=False):
        logger.info("\nğŸ“Š EVALUATING OPTIMIZED PROMPT ğŸ“Š\n")
        # è·å–ç›®å‰æœ€å¥½çš„ç»“æœ
        # best_samples = self.data_utils.get_best_round()
        # ç”¨æœ€æ–°ç»“æœå’Œä¸Šä¸€æ¬¡ç»“æœè¿›è¡Œæ¯”è¾ƒè¯„ä¼°
        success, modification_all, anaysis_all = self.evaluation_utils.evaluate_prompt(
            self, last_samples, new_samples, qa_list, initial=initial
        )
        # ä¿å­˜è¯„ä¼°ç»“æœï¼ˆæœ‰å¯èƒ½successä¸ºfalseï¼Œä¿å­˜çš„å¹¶ä¸ä¸€å®šä¸ºæœ€ä½³ï¼‰
        # directory = self.prompt_utils.create_round_directory(self.root_path, self.round)
        # self.prompt_utils.write_answers(directory, success, modification_all, answers=answers)

        return modification_all, anaysis_all, success
