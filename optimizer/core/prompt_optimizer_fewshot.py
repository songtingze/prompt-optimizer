from typing import Dict, List, Optional, Tuple, Callable, Any
import json
from pathlib import Path
from llm.llm import LLM
from optimizer.prompts.system_prompts import REFLECTION_TEMPLATE1, OPTIMIZATION_TEMPLATE1
from llm.llm_config import LLM_Config
from datetime import datetime
from optimizer.utils import load
from optimizer.utils.prompt_utils import PromptUtils
from optimizer.utils.evaluation_utils import EvaluationUtils
from optimizer.utils.data_utils import DataUtils


class PromptOptimizer:
    def __init__(
            self,
            optimize_llm_config: LLM_Config,
            evaluate_llm_config: LLM_Config,
            execute_llm_config: LLM_Config,
            rounds: int,
            optimized_path: str = None,
            name: str = "",
            template: str = ""
    ):
        """
        åˆå§‹åŒ–æç¤ºè¯ä¼˜åŒ–å™¨

        Args:
            model_name: æ¨¡å‹åç§°
            api_key: APIå¯†é’¥
            base_url: APIåŸºç¡€URL
            temperature: æ¨¡å‹temperature
            **kwargs: å…¶ä»–æ¨¡å‹å‚æ•°
        """
        self.optimize_llm = LLM(
            model_name=optimize_llm_config.model_name,
            api_key=optimize_llm_config.api_key,
            base_url=optimize_llm_config.base_url,
            temperature=optimize_llm_config.temperature
        )

        self.evaluate_llm = LLM(
            model_name=evaluate_llm_config.model_name,
            api_key=evaluate_llm_config.api_key,
            base_url=evaluate_llm_config.base_url,
            temperature=evaluate_llm_config.temperature
        )

        self.execute_llm = LLM(
            model_name=execute_llm_config.model_name,
            api_key=execute_llm_config.api_key,
            base_url=execute_llm_config.base_url,
            temperature=execute_llm_config.temperature
        )

        self.name = name
        self.root_path = Path(optimized_path) / self.name
        self.top_scores = []
        self.round = rounds
        # self.max_rounds = max_rounds
        self.template = template
        self.prompt_utils = PromptUtils(self.root_path)
        self.evaluation_utils = EvaluationUtils(self.root_path)
        self.data_utils = DataUtils(self.root_path)
        # self.evaluation_utils = EvaluationUtils(self.root_path)

    # ä¼˜åŒ–æç¤ºè¯
    def optimize_prompt(
            self,
            prompt: str,
            optimize_suggestion: str,
    ) -> Tuple[str, int]:
        # ç”Ÿæˆä¼˜åŒ–åçš„æç¤ºè¯
        optimization_prompt = OPTIMIZATION_TEMPLATE1.format(
            original_prompt=prompt,
            optimize_suggestion=optimize_suggestion,
        )

        optimized_prompt, tokens = self.optimize_llm.generate_response(optimization_prompt)
        print(f"ä¼˜åŒ–çš„æç¤ºè¯ï¼š\n{optimized_prompt}")
        return optimized_prompt, tokens

    # å®Œæ•´ä¼˜åŒ–è¿‡ç¨‹
    def optimize(self, current_prompt):
        # 2.æ‰§è¡Œä¼˜åŒ–åçš„prompt
        new_samples = self._execute_prompt()
        # 3.è¯„ä¼°å¹¶åæ€
        self._evaluate_prompt(new_samples)
        # 1.ä¼˜åŒ–ï¼ˆå¦‚æœæ˜¯ç¬¬ä¸€è½®åˆ™è·³è¿‡è¯¥æ­¥éª¤ï¼‰
        self._optimize_prompt(current_prompt)

        self.round += 1
        self.show_final_result()
        return self.round

    def _optimize_prompt(self, current_prompt):
        # prompt_path = self.root_path / "prompts"
        # load.set_file_name(self.template)

        if self.round == 1:
            # ç¬¬ä¸€æ¬¡è·³è¿‡ä¼˜åŒ–
            self._handle_first_round_op(self.root_path)
        else:
            # æ‰§è¡Œä¼˜åŒ–
            directory = self.prompt_utils.create_round_directory(self.root_path, self.round)
            new_prompt = self._generate_optimized_prompt()
            self.prompt = new_prompt
            print(f"\nRound {self.round} Prompt: {self.prompt}\n")
            # è®°å½•è¯¥è½®ä¼˜åŒ–åçš„prompt
            self.prompt_utils.write_prompt(directory, prompt=self.prompt)
        return self.prompt

    def show_final_result(self):
        pass

    def _handle_first_round_op(self, prompt_path: Path, data: List[dict]) -> None:
        print("\nâš¡ RUNNING Round 1 PROMPT âš¡\n")
        directory = self.prompt_utils.create_round_directory(prompt_path, self.round)

        # è¯»å–åˆå§‹åŒ–é…ç½®æ¨¡æ¿
        prompt, _, _, _ = load.load_meta_data()
        # è®°å½•ç¬¬ä¸€è½®prompt
        self.prompt = prompt
        self.prompt_utils.write_prompt(directory, prompt=self.prompt)

        new_samples = await self.evaluation_utils.execute_prompt(self, directory)
        _, answers = await self.evaluation_utils.evaluate_prompt(
            self, None, new_samples, path=prompt_path, data=data, initial=True
        )
        self.prompt_utils.write_answers(directory, answers=answers)

    def _generate_optimized_prompt(self):
        pass

    def _execute_prompt(self):
        load.set_file_name(self.template)
        # ç¬¬ä¸€è½®æ²¡æœ‰ä¼˜åŒ–åçš„prompt
        if self.round == 1:
            # è¯»å–åˆå§‹åŒ–é…ç½®æ¨¡æ¿
            prompt, _, _, _ = load.load_meta_data()
            # åˆå§‹è¾“å…¥prompt
            self.prompt = prompt
            current_prompt = self.prompt
        else:
            # è¯»å–ä¸Šä¸€è½®ä¼˜åŒ–åçš„prompt
            directory = self.prompt_utils.create_round_directory(self.root_path, self.round - 1)
            current_prompt = self.prompt_utils.load_prompt(self.round - 1, directory)
        print("\nâš¡ RUNNING OPTIMIZED PROMPT âš¡\n")

        # æ‰§è¡Œprompt
        new_samples = self.evaluation_utils.execute_prompt(self, current_prompt)
        return new_samples

    def _evaluate_prompt(self, new_samples):
        print("\nğŸ“Š EVALUATING OPTIMIZED PROMPT ğŸ“Š\n")
        # è·å–ç›®å‰æœ€å¥½çš„ç»“æœ
        samples = self.data_utils.get_best_round()
        # ç”¨æœ€æ–°ç»“æœå’Œæœ€å¥½ç»“æœè¿›è¡Œæ¯”è¾ƒè¯„ä¼°
        success, answers = await self.evaluation_utils.evaluate_prompt(
            self, samples, new_samples, path=self.root_path, data=data, initial=(self.round == 1)
        )

        self.prompt_utils.write_answers(directory, answers=answers)
