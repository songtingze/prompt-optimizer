import argparse
from typing import Tuple

from optimizer.core.prompt_optimizer_fewshot import PromptOptimizer
from llm.llm_config import LLM_Config
import yaml


def parse_args():
    parser = argparse.ArgumentParser(description="PromptOptimizer CLI")

    # LLM parameter
    parser.add_argument("--opt-model", type=str, default="deepseek-r1", help="Model for optimization")
    parser.add_argument("--opt-temp", type=float, default=0.7, help="Temperature for optimization")
    parser.add_argument("--eval-model", type=str, default="deepseek-r1", help="Model for evaluation")
    parser.add_argument("--eval-temp", type=float, default=0.3, help="Temperature for evaluation")
    parser.add_argument("--exec-model", type=str, default="deepseek-r1", help="Model for execution")
    parser.add_argument("--exec-temp", type=float, default=0.4, help="Temperature for execution")

    # PromptOptimizer parameter
    parser.add_argument("--workspace", type=str, default="../../results", help="Path for optimized output")
    parser.add_argument("--current_round", type=int, default=1, help="Initial round number")
    parser.add_argument("--max-rounds", type=int, default=10, help="Maximum number of rounds")
    parser.add_argument("--template", type=str, default="SummaryCJ.yaml", help="Template file name")
    parser.add_argument("--name", type=str, default="SummaryCJ", help="Project name")

    return parser.parse_args()


def load_config(config_path: str) -> dict:
    """ä» YAML æ–‡ä»¶åŠ è½½é…ç½®"""
    with open(config_path, 'r') as f:
        return yaml.safe_load(f)


def create_llm_config_from_yaml(
        config: dict,
        model_name: str,
        temperature: float
) -> LLM_Config:
    """
    æ ¹æ® YAML ä¸­çš„é…ç½®åˆ›å»º LLM_Config å®ä¾‹
    - å¦‚æœæŒ‡å®šäº† model_nameï¼Œåˆ™ä» models å­—æ®µä¸­è¯»å–
    - å¦åˆ™ä½¿ç”¨å…¨å±€ llm å­—æ®µä½œä¸ºé»˜è®¤é…ç½®
    """

    if model_name in config.get("models", {}):
        model_config = config["models"][model_name]

    else:
        # ä½¿ç”¨é»˜è®¤
        model_config = config["llm"]

    return LLM_Config(
        model_name=model_name,
        api_key=model_config["api_key"],
        base_url=model_config["base_url"],
        temperature=temperature
    )


# å¯åŠ¨ç¬¬ä¸€è½®ä¼˜åŒ–
def start_optimization(optimizer) -> Tuple[str, str, int, str, bool]:

    rt_llm_feedback, rt_prompt, c_round, fn_answer, success = optimizer.optimize_first()

    return rt_llm_feedback, rt_prompt, c_round, fn_answer, success


# ç»§ç»­ä¼˜åŒ–
def continue_optimization(optimizer, current_round, user_feedback: str, llm_feedback:str, best_answer, best_prompt) -> Tuple[str, str, int, str, bool]:
    # è°ƒç”¨ä¸‹ä¸€è½®ä¼˜åŒ–
    llm_feedback, current_prompt, next_round, fn_answer, success = optimizer.optimize_next(current_round, user_feedback, llm_feedback, best_answer, best_prompt)

    return llm_feedback, current_prompt, next_round, fn_answer, success

if __name__ == "__main__":
    args = parse_args()

    config = load_config("../../llm/config.yaml")

    # æ„å»º LLM é…ç½®
    optimize_config = create_llm_config_from_yaml(config, args.opt_model, args.opt_temp)
    evaluate_config = create_llm_config_from_yaml(config, args.eval_model, args.eval_temp)
    execute_config = create_llm_config_from_yaml(config, args.exec_model, args.exec_temp)

    # åˆå§‹åŒ–ä¼˜åŒ–ã€æ‰§è¡Œã€è¯„ä¼°åæ€ä¸‰é˜¶æ®µä¼˜åŒ–å™¨è®¾ç½®
    optimizer = PromptOptimizer(
        optimize_llm_config=optimize_config,
        evaluate_llm_config=evaluate_config,
        execute_llm_config=execute_config,
        rounds=args.current_round,
        optimized_path=args.workspace,
        template=args.template,
        name=args.name
    )

    # å¯åŠ¨ç¬¬ä¸€è½®ä¼˜åŒ–
    llm_feedback, current_prompt, current_round, current_answer, success = start_optimization(optimizer)

    print(f"ğŸš© +++++++++è¿”å›å‰ç«¯æ¥å£ç»“æœ+++round: {current_round-1}++++++ ğŸš©")
    print("è¿”å›çš„å¤§æ¨¡å‹ä¼˜åŒ–åæ€ï¼š", llm_feedback)
    print("è¿”å›çš„å½“å‰ä¼˜åŒ–åçš„promptï¼š", current_prompt)
    print("è¿”å›çš„å½“å‰ä¼˜åŒ–åçš„é—®é¢˜æ‰§è¡Œç»“æœï¼š", current_answer)
    print("å½“å‰ä¼˜åŒ–æ˜¯å¦æˆåŠŸï¼š", success)
    print("++++++++++++++++++++++++++++++++++++++")

    # å¯åŠ¨ä¼˜åŒ–è¿­ä»£
    while True:
        user_feedback = input("è¯·ç”¨æˆ·æ ¹æ®ä¸Šä¸€è½®çš„ä¼˜åŒ–ç»“æœï¼Œè¾“å…¥ä½ çš„ä¼˜åŒ–å»ºè®®ï¼ˆè¾“å…¥ 'exit' é€€å‡ºï¼›æ²¡æœ‰å»ºè®®åˆ™è¾“å…¥ç©ºå­—ç¬¦ä¸²''ï¼‰: ")
        if user_feedback.lower() == "exit":
            break

        llm_feedback, current_prompt, current_round, current_answer, success = continue_optimization(
            optimizer, current_round+1, user_feedback, llm_feedback, current_answer, current_prompt)

        print(f"ğŸš© +++++++++è¿”å›å‰ç«¯æ¥å£ç»“æœ+++round: {current_round - 1}++++++ ğŸš©\n")
        print("è¿”å›çš„å¤§æ¨¡å‹ä¼˜åŒ–åæ€ï¼š", llm_feedback, "\n")
        print("è¿”å›çš„å½“å‰ä¼˜åŒ–åçš„promptï¼š", current_prompt, "\n")
        print("è¿”å›çš„å½“å‰ä¼˜åŒ–åçš„é—®é¢˜æ‰§è¡Œç»“æœï¼š", current_answer, "\n")
        print("å½“å‰ä¼˜åŒ–æ˜¯å¦æˆåŠŸï¼š", success)
        print("++++++++++++++++++++++++++++++++++++++")


